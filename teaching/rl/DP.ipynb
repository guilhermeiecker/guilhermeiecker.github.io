{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>In-Class Exercises: Dynamic Programming</h1>\n",
    "\n",
    "This is a set of exercises to help you visualize the concepts introduced in class.\n",
    "\n",
    "<h2>1. Gridworld </h2>\n",
    "\n",
    "The exercises are based on the Gridworld game, which comprises the following rules:<br>\n",
    "- There is a squared grid with $K$ tiles in each orientation. <br>\n",
    "- The player (agent) is free to move throughout the grid in 4 different directions: North (N), South (S), East (E), and West (W). <br>\n",
    "- At the boundary tiles, an attempt to move outside the grid results in returning to the current tile. <br>\n",
    "- There is at least one \"final\" tile and the game is over when the agent reaches one of them. <br>\n",
    "- Our goal is to reach a final tile as quick as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2.Markov Decision Processes (MDPs) </h2>\n",
    "\n",
    "The first step is to represent the problem as an MDP with the following components: <br>\n",
    "- Set of states: $\\mathcal S=\\{(x,y)\\}_{x=1,\\dots,K, y=1,\\dots,K}$ (each tile is a state) <br>\n",
    "- Set of actions $\\mathcal A=\\{\\uparrow, \\downarrow, \\rightarrow, \\leftarrow\\}$ (the same for each state) <br>\n",
    "- Discount Factor: $\\gamma$\n",
    "- States $(1,1)$ and $(K,K)$ are (absorbing) final states\n",
    "\n",
    "Because, in this example, all actions lead deterministically to a single state, we consider: <br>\n",
    "- Transition Probabilities: $\\mathcal P_{ss'}^{a} = \\begin{cases} 1, & \\text{ if } a(s)=s' \\\\ 0, & \\text{otherwise}\\end{cases}$<br>\n",
    "- Reward: $\\mathcal R_s^a = \\mathbb{E} [ R_{t+1} |\\, S_t=s, A_t=a] = r = -1$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self, grid_size=3, gamma=1.0, reward=-1):\n",
    "        self.k = grid_size  # grid side\n",
    "        self.g = gamma      # discount factor\n",
    "        self.r = reward     # fixed reward\n",
    "\n",
    "        # set of states\n",
    "        self.S  = [(i+1,j+1) for i in range(self.k) for j in range(self.k)]   \n",
    "\n",
    "        # set of actions\n",
    "        self.A  = [\"N\", \"S\", \"E\", \"W\"]                                            \n",
    "\n",
    "        # definition of expected reward R\n",
    "        self.R  = { (s, a): 0 if (s==(1,1) or s==(self.k, self.k)) else self.r for s in self.S for a in self.A }                \n",
    "\n",
    "        # definition of transition probabilities\n",
    "        self.Tp = { (s, ss, a): 1 if self.action(s,a) == ss else 0 for s in self.S for ss in self.S for a in self.A}  \n",
    "    \n",
    "    # support function \"action\" that help define the transition probabilities Tp\n",
    "    def action(self, s, a):   \n",
    "        if s == (1,1):\n",
    "            return s\n",
    "        if s == (self.k, self.k):\n",
    "            return (1,1)\n",
    "        if a == \"N\":\n",
    "            return (max(1, s[0]-1), s[1])\n",
    "        if a == \"S\":\n",
    "            return (min(self.k, s[0]+1), s[1])\n",
    "        if a == \"E\":\n",
    "            return (s[0], min(self.k, s[1]+1))\n",
    "        if a == \"W\":\n",
    "            return (s[0], max(1, s[1]-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3.Policy Evaluation </h2>\n",
    "\n",
    "The idea is to use the Dynamic Programming framework. In our case, we will use Bellman expectation equation iteravitely to compute the value function of each state in our Gridworld MDP.\n",
    "\n",
    "<b><u>Exercise 2:</b></u> Provide the line of code to calculate the current iteration of the Bellman Expectation Equation for each state. Answer directly on the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Evaluation Algorithm\n",
    "# Input:  MDP parameters (states S, actions A, rewards R, transition probabilities Tp)\n",
    "#         policy Pi\n",
    "#         initial value v0\n",
    "#         maximum number of iterations max_iter\n",
    "# Output: list of the max_iter values for the states (last item of the list estimates v_pi)\n",
    "def policy_eval(mdp, Pi, v0, max_iter=100):\n",
    "    v = [v0]\n",
    "    for i in range(1, max_iter+1):\n",
    "        v.append({})\n",
    "        for s in mdp.S:\n",
    "            v[i][s] = sum( Pi[(s,a)] * (mdp.R[(s,a)] + mdp.g*sum(mdp.Tp[(s,ss,a)]*v[i-1][ss] for ss in mdp.S)) for a in mdp.A)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.1. Evaluating a <b>Random Policy</b> </h3>\n",
    "\n",
    "Uniform random policy: $\\pi(\\uparrow |\\,\\cdot) = \\pi(\\rightarrow |\\,\\cdot) = \\pi(\\downarrow |\\,\\cdot) = \\pi(\\leftarrow|\\,\\cdot) = 0.25$\n",
    "\n",
    "<b><u>Exercise 3:</b></u> How many iterations do we need to converge to the value of the random policy? (you can test the results by changing the assignment of variable max_iter) Answer: 106 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the values of the random policy\n",
      " 0.00000\t-7.00000\t-9.00000\n",
      "-7.00000\t-8.00000\t-7.00000\n",
      "-9.00000\t-7.00000\t 0.00000\n"
     ]
    }
   ],
   "source": [
    "# instantiation of an MDP object with default parameters\n",
    "mdp = MDP() \n",
    "\n",
    "# definition of policy pi\n",
    "randomPi = { (s, a): 1.0/len(mdp.A) for s in mdp.S for a in mdp.A }   \n",
    "\n",
    "# definition of the state-value function\n",
    "v0 = {s: 0 for s in mdp.S}\n",
    "\n",
    "# maximum number of iterations\n",
    "max_iter=106\n",
    "\n",
    "# evaluate policy Pi over max_iter iterations and store value functions of every iteration\n",
    "v = policy_eval(mdp, randomPi, v0, max_iter)\n",
    "\n",
    "# take the values of the last iteration, i.e., the best estimate of the value of pi\n",
    "vpi = v[-1]\n",
    "\n",
    "## print results\n",
    "print(\"These are the values of the random policy\")\n",
    "for s in mdp.S:\n",
    "    print(\" %.5f\" %vpi[s] if vpi[s]==0 else \"%.5f\" %vpi[s], end='\\n' if s[1] % mdp.k == 0 else '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.2. Evaluating an <b>Oracle Policy</b> </h3>\n",
    "\n",
    "<b><u>Exercise 4:</b></u> Given that we, as external observers, know which states are important for win the Gridworld game, replace the policy $oraclePi$ below to instruct the agent to take the decision you judge to be the best one for every state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the values of the oracle policy\n",
      " 0.00000\t-1.00000\t-2.00000\n",
      "-1.00000\t-2.00000\t-1.00000\n",
      "-2.00000\t-1.00000\t 0.00000\n"
     ]
    }
   ],
   "source": [
    "# instantiation of an MDP object with default parameters\n",
    "mdp = MDP() \n",
    "\n",
    "# definition of custom policy pi\n",
    "# TODO: The policy below behaves exactly like the random policy. Replace the probability of each action with a value you think suits best\n",
    "oraclePi = {\n",
    "    ((1,1), \"N\"): 1.0,\n",
    "    ((1,1), \"S\"): 0.0,\n",
    "    ((1,1), \"E\"): 0.0,\n",
    "    ((1,1), \"W\"): 0.0,\n",
    "\n",
    "    ((1,2), \"N\"): 0.0,\n",
    "    ((1,2), \"S\"): 0.0,\n",
    "    ((1,2), \"E\"): 0.0,\n",
    "    ((1,2), \"W\"): 1.0,\n",
    "\n",
    "    ((1,3), \"N\"): 0.0,\n",
    "    ((1,3), \"S\"): 0.0,\n",
    "    ((1,3), \"E\"): 0.0,\n",
    "    ((1,3), \"W\"): 1.0,\n",
    "\n",
    "    ((2,1), \"N\"): 1.0,\n",
    "    ((2,1), \"S\"): 0.0,\n",
    "    ((2,1), \"E\"): 0.0,\n",
    "    ((2,1), \"W\"): 0.0,\n",
    "\n",
    "    ((2,2), \"N\"): 1.0,\n",
    "    ((2,2), \"S\"): 0.0,\n",
    "    ((2,2), \"E\"): 0.0,\n",
    "    ((2,2), \"W\"): 0.0,\n",
    "\n",
    "    ((2,3), \"N\"): 0.0,\n",
    "    ((2,3), \"S\"): 1.0,\n",
    "    ((2,3), \"E\"): 0.0,\n",
    "    ((2,3), \"W\"): 0.0,\n",
    "\n",
    "    ((3,1), \"N\"): 0.0,\n",
    "    ((3,1), \"S\"): 0.0,\n",
    "    ((3,1), \"E\"): 1.0,\n",
    "    ((3,1), \"W\"): 0.0,\n",
    "\n",
    "    ((3,2), \"N\"): 0.0,\n",
    "    ((3,2), \"S\"): 0.0,\n",
    "    ((3,2), \"E\"): 1.0,\n",
    "    ((3,2), \"W\"): 0.0,\n",
    "\n",
    "    ((3,3), \"N\"): 1.0,\n",
    "    ((3,3), \"S\"): 0.0,\n",
    "    ((3,3), \"E\"): 0.0,\n",
    "    ((3,3), \"W\"): 0.0,\n",
    "}\n",
    "\n",
    "# definition of the state-value function\n",
    "v0 = {s: 0 for s in mdp.S}\n",
    "\n",
    "# maximum number of iterations\n",
    "max_iter=300\n",
    "\n",
    "# evaluate policy Pi over max_iter iterations and store value functions of every iteration\n",
    "v = policy_eval(mdp, oraclePi, v0, max_iter)\n",
    "\n",
    "# take the values of the last iteration, i.e., the best estimate of the value of pi\n",
    "vpi = v[-1]\n",
    "\n",
    "## print results\n",
    "print(\"These are the values of the oracle policy\")\n",
    "for s in mdp.S:\n",
    "    print(\" %.5f\" %vpi[s] if vpi[s]==0 else \"%.5f\" %vpi[s], end='\\n' if s[1] % mdp.k == 0 else '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4. Policy Iteration </h2>\n",
    "\n",
    "In Policy Iteration, we (i) evaluate and (ii) greedily improve the initial policy.<br>\n",
    "We iterate until we observe convergence ($\\epsilon$-convergence) or until we reach the maximum number of iterations ($maxIter$). <br>\n",
    "Before we see how Policy Iteration works, since we have already covered part (i) Policy Evaluation, let's discuss part (ii) Policy Improvement.\n",
    "\n",
    " <h3> 4.1. Policy Improvement </h3>\n",
    "\n",
    " The idea of policy improvement is to find a new policy $\\pi'$ based on an initial policy $\\pi$, such that $v_{\\pi'}(s) \\geq v_{\\pi}(s), \\forall s\\in S$. \n",
    " The simplest way of doing that is through greedy improvements of the initial policy.\n",
    " If we evaluate the value function of all states of the MDP according to the initial policy, i.e., $v_{\\pi}$, we can define the $\\pi'$ such that the agent`s action is to choose the state with the highest value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Improvement Algorithm\n",
    "# Input:  MDP parameters (states S, actions A)\n",
    "#         initial policy\n",
    "# Output: improved policy (deterministic greedy action)\n",
    "def policy_improv(mdp, v):\n",
    "    newPi = {}\n",
    "    for s in mdp.S:\n",
    "        best_v = float('-inf')\n",
    "        best_action = \"N\"\n",
    "        for a in mdp.A:\n",
    "            neighbor = mdp.action(s, a)\n",
    "            if v[neighbor] >= best_v:\n",
    "                best_v = v[neighbor]\n",
    "                best_action = a\n",
    "        for a in mdp.A:\n",
    "            newPi[s, a] = 1.0 if a == best_action else 0.0\n",
    "    return newPi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test the policy improvement function using the random policy as our initial policy.\n",
    "\n",
    "<b><u>Exercise 5:</b></u> Fill the lines for variables $Pi new$ and $v new$ in order to obtain the improved values of the original random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the values of the initial policy\n",
      " 0.00000\t-7.00000\t-9.00000\n",
      "-7.00000\t-8.00000\t-7.00000\n",
      "-9.00000\t-7.00000\t 0.00000\n",
      "\n",
      "These are the values of the improved policy\n",
      " 0.00000\t-1.00000\t-2.00000\n",
      "-1.00000\t-2.00000\t-1.00000\n",
      "-2.00000\t-1.00000\t 0.00000\n"
     ]
    }
   ],
   "source": [
    "# instantiation of an MDP object with default parameters\n",
    "mdp = MDP() \n",
    "\n",
    "# initial (random) policy\n",
    "Pi_old = { (s, a): 1.0/len(mdp.A) for s in mdp.S for a in mdp.A }   \n",
    "\n",
    "# find the value of the initial policy\n",
    "v_old = policy_eval(mdp, Pi_old, v0 = {s: 0 for s in mdp.S}, max_iter=300)[-1]\n",
    "\n",
    "# compute new policy based on the improvement of the initial policy\n",
    "Pi_new = policy_improv(mdp, v_old)\n",
    "\n",
    "# find the value of the improved policy\n",
    "v_new = policy_eval(mdp, Pi_new, v0 = {s: 0 for s in mdp.S}, max_iter=300)[-1]\n",
    "\n",
    "## print results\n",
    "print(\"These are the values of the initial policy\")\n",
    "for s in mdp.S:\n",
    "    print(\" %.5f\" %v_old[s] if v_old[s]==0 else \"%.5f\" %v_old[s], end='\\n' if s[1] % mdp.k == 0 else '\\t')\n",
    "\n",
    "print(\"\\nThese are the values of the improved policy\")\n",
    "for s in mdp.S:\n",
    "    print(\" %.5f\" %v_new[s] if v_new[s]==0 else \"%.5f\" %v_new[s], end='\\n' if s[1] % mdp.k == 0 else '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 4.2. Policy Iteration function </h3>\n",
    "Finally, we have all the elements to implement the Policy Iteration Algorithm as a Python function.\n",
    "\n",
    "<b><u>Exercise 6:</b></u> Fill the code below to complete the logic of the Policy Iteration algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Iteration Algorithm\n",
    "# Input:  MDP parameters (states S, actions A)\n",
    "#         initial policy, Pi_initial\n",
    "#         maximum number of iterations, max_Iter\n",
    "# Output: (estimate of the) optimal policy\n",
    "def policy_iteration(mdp, Pi_initial, max_iter):\n",
    "    Pi_opt = Pi_initial\n",
    "    for i in range(max_iter):\n",
    "        v = policy_eval(mdp, Pi_opt, v0 = {s: 0 for s in mdp.S}, max_iter=300)[-1]\n",
    "        Pi_opt = policy_improv(mdp, v)\n",
    "    return Pi_opt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 4.3. Testing Policy Iteration on a more complex Gridworld </h3>\n",
    "\n",
    "Consider a new version of the $K$-Gridworld where the 2 final states are now granting the following rewards:<br>\n",
    "- State $(1,1)$ is a pure-win state, where $r=0$, as before; and <br>\n",
    "- State $(K,K)$ is a damaged-win state, where $r=-4K$.\n",
    "\n",
    "If the agent reaches state $(K,K)$, it is penalized by $K$ units and it finishes the episode by moving directly to the absorbing state $(1,1)$.\n",
    "We can change by simply updating the rewards as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the values of the initial random policy\n",
      " 0.00000\t-11.00000\t-15.00000\n",
      "-11.00000\t-14.00000\t-15.00000\n",
      "-15.00000\t-15.00000\t-12.00000\n",
      "\n",
      "These are the values of the 1-step improved policy\n",
      " 0.00000\t-1.00000\t-2.00000\n",
      "-1.00000\t-2.00000\t-13.00000\n",
      "-2.00000\t-13.00000\t-12.00000\n",
      "\n",
      "These are the values of the optimal policy\n",
      " 0.00000\t-1.00000\t-2.00000\n",
      "-1.00000\t-2.00000\t-3.00000\n",
      "-2.00000\t-3.00000\t-12.00000\n"
     ]
    }
   ],
   "source": [
    "mdp = MDP(grid_size=3, gamma=1.0, reward=-1)\n",
    "for a in mdp.A:\n",
    "    mdp.R[((mdp.k, mdp.k), a)] = -4*mdp.k\n",
    "\n",
    "# initial (random) policy\n",
    "Pi_init = { (s, a): 1.0/len(mdp.A) for s in mdp.S for a in mdp.A } \n",
    "\n",
    "# evaluate random policy\n",
    "v_init = policy_eval(mdp, Pi_init, v0 = {s: 0 for s in mdp.S}, max_iter=300)[-1]\n",
    "\n",
    "# print value of random policy\n",
    "print(\"These are the values of the initial random policy\")\n",
    "for s in mdp.S:\n",
    "    print(\" %.5f\" %v_init[s] if v_init[s]==0 else \"%.5f\" %v_init[s], end='\\n' if s[1] % mdp.k == 0 else '\\t')\n",
    "\n",
    "# compute 1-step improved policy\n",
    "Pi_impr = policy_improv(mdp, v_init)\n",
    "\n",
    "# evaluate 1-step improved policy\n",
    "v_impr = policy_eval(mdp, Pi_impr, v0 = {s: 0 for s in mdp.S}, max_iter=300)[-1]\n",
    "\n",
    "# print value of 1-step improved policy\n",
    "print(\"\\nThese are the values of the 1-step improved policy\")\n",
    "for s in mdp.S:\n",
    "    print(\" %.5f\" %v_impr[s] if v_impr[s]==0 else \"%.5f\" %v_impr[s], end='\\n' if s[1] % mdp.k == 0 else '\\t')\n",
    "\n",
    "# compute optimal policy by Policy Iteration\n",
    "Pi_opt = policy_iteration(mdp, Pi_init, max_iter=300)\n",
    "\n",
    "# evaluate optimal policy\n",
    "v_opt = policy_eval(mdp, Pi_opt, v0 = {s: 0 for s in mdp.S}, max_iter=300)[-1]\n",
    "\n",
    "# print value of 1-step improved policy\n",
    "print(\"\\nThese are the values of the optimal policy\")\n",
    "for s in mdp.S:\n",
    "    print(\" %.5f\" %v_opt[s] if v_opt[s]==0 else \"%.5f\" %v_opt[s], end='\\n' if s[1] % mdp.k == 0 else '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 5. Value Iteration </h2>\n",
    "\n",
    "Value Iteration is a second method to \"solve\" MDPs using Dynamic Programming. The idea is to iteratively apply Bellman Optimality Equation, which computes the states' values by considering greedy decisions. At the end of the process, we obtain the optimal value of each state and, consequently, the optimal policy.\n",
    "\n",
    "<h3> 5.1. Value Iteration Function </h3>\n",
    "\n",
    "<b><u>Exercise 7:</b></u> Provide the line of code to calculate the current iteration of the Bellman Optimality Equation for each state. Answer directly on the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration Algorithm\n",
    "# Input:  MDP parameters (states S, actions A)\n",
    "# Output: (Estimate of the) Optimal values\n",
    "def value_iteration(mdp, max_iter=100):\n",
    "    v = [{s: 0 for s in mdp.S}]\n",
    "    for i in range(1, max_iter+1):\n",
    "        v.append({})\n",
    "        for s in mdp.S:\n",
    "            v[i][s] = max((mdp.R[(s,a)] + mdp.g*sum(mdp.Tp[(s,ss,a)]*v[i-1][ss] for ss in mdp.S)) for a in mdp.A)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5.2. Testing Value Iteration </h3>\n",
    "\n",
    "<b><u>Exercise 8:</b></u> Find the minimium number of iterations for both Policy Iteration (PI) and Value Iteration (VI) to convert to the optimal solution. Observe the differences between number of iterations and runtime between both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration took 0.952484 s.\n",
      "These are its values after 5 iterations:\n",
      " 0.00000\t-1.00000\t-2.00000\t-3.00000\t-4.00000\t-5.00000\n",
      "-1.00000\t-2.00000\t-3.00000\t-4.00000\t-5.00000\t-6.00000\n",
      "-2.00000\t-3.00000\t-4.00000\t-5.00000\t-6.00000\t-7.00000\n",
      "-3.00000\t-4.00000\t-5.00000\t-6.00000\t-7.00000\t-8.00000\n",
      "-4.00000\t-5.00000\t-6.00000\t-7.00000\t-8.00000\t-9.00000\n",
      "-5.00000\t-6.00000\t-7.00000\t-8.00000\t-9.00000\t-24.00000\n",
      "Value Iteration took 0.005910 s.\n",
      "These are its values after 9 iterations:\n",
      " 0.00000\t-1.00000\t-2.00000\t-3.00000\t-4.00000\t-5.00000\n",
      "-1.00000\t-2.00000\t-3.00000\t-4.00000\t-5.00000\t-6.00000\n",
      "-2.00000\t-3.00000\t-4.00000\t-5.00000\t-6.00000\t-7.00000\n",
      "-3.00000\t-4.00000\t-5.00000\t-6.00000\t-7.00000\t-8.00000\n",
      "-4.00000\t-5.00000\t-6.00000\t-7.00000\t-8.00000\t-9.00000\n",
      "-5.00000\t-6.00000\t-7.00000\t-8.00000\t-9.00000\t-24.00000\n"
     ]
    }
   ],
   "source": [
    "num_of_iterations_PI = 5\n",
    "num_of_iterations_VI = 9\n",
    "\n",
    "mdp5 = MDP(grid_size=6, gamma=1.0, reward=-1)\n",
    "for a in mdp5.A:\n",
    "    mdp5.R[((mdp5.k, mdp5.k), a)] = -4*mdp5.k\n",
    "\n",
    "# initial (random) policy\n",
    "Pi_init = { (s, a): 1.0/len(mdp5.A) for s in mdp5.S for a in mdp5.A } \n",
    "\n",
    "# compute optimal policy by Policy Iteration\n",
    "pi_start = time.time()\n",
    "Pi_PI = policy_iteration(mdp5, Pi_init, max_iter=num_of_iterations_PI)\n",
    "pi_end = time.time()\n",
    "\n",
    "# evaluate optimal policy\n",
    "v_PI = policy_eval(mdp5, Pi_PI, v0 = {s: 0 for s in mdp5.S}, max_iter=300)[-1]\n",
    "\n",
    "# compute optimal value for the MDP\n",
    "vi_start = time.time()\n",
    "v_VI = value_iteration(mdp5, max_iter=num_of_iterations_VI)[-1]\n",
    "vi_end = time.time()\n",
    "\n",
    "# print optimal value obtained by Policy Iteration\n",
    "print(\"Policy Iteration took %f s.\" %(pi_end - pi_start))\n",
    "print(\"These are its values after %d iterations:\" %num_of_iterations_PI)\n",
    "for s in mdp5.S:\n",
    "    print(\" %.5f\" %v_PI[s] if v_PI[s]==0 else \"%.5f\" %v_PI[s], end='\\n' if s[1] % mdp5.k == 0 else '\\t')\n",
    "\n",
    "# print optimal value obtained by Value Iteration\n",
    "print(\"Value Iteration took %f s.\" %(vi_end - vi_start))\n",
    "print(\"These are its values after %d iterations:\" %num_of_iterations_VI)\n",
    "for s in mdp5.S:\n",
    "    print(\" %.5f\" %v_VI[s] if v_VI[s]==0 else \"%.5f\" %v_VI[s], end='\\n' if s[1] % mdp5.k == 0 else '\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
