{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>TP: Dynamic Programming</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1. Introduction: Packet Scheduling</h2>\n",
    "\n",
    "Consider a router handling packets from flows of $K$ different priority classes.<br>\n",
    "At a given moment, there is a fixed number $N_k$ of packets for each class $k=1,\\dots,K$. <br>\n",
    "All packets have the same length $L$ [bits], but packets of priority class $k$ have transmission rate of $R_k$ [bps], such that $R_1 > R_2 > \\dots > R_K$. <br>\n",
    "Therefore, a packet of class $k$ takes $T_k=L / R_k$ [s] to be transmitted.\n",
    "\n",
    "Packets are organized in a queue $Q_t$ that changes at every iteration $t$, depending on which packet was chosen to be transmitted, i.e., the scheduling policy. <br>\n",
    "At every iteration $t$, the router selects a packet of a given class $k$ to be transmitted, which takes $T_k$ to finish the transmission. <br>\n",
    "This means that the remaining packets in the queue must wait $T_k$ before having another chance to be transmitted. <br>\n",
    "We define the worst-case (queue) wait time of all packets in the queue $Q_t$ at iteration $t$ as: <br>\n",
    "\n",
    "$W_t = \\sum_{p\\in Q_t} T_{k(p)}$, where $k(p)$ is the class of packet $p$ <br>\n",
    "\n",
    "\n",
    "The goal of this activity is to find a <u><b>scheduling policy</b></u> $\\pi^*$ that is able to <u><b>minimize the total worst-case wait time over all iterations until all packets are transmitted</u></b>, i.e.,<br>\n",
    "\n",
    "$\\min \\sum_{t=1}^{\\infty} W_t = \\sum_{t=1}^{\\infty} \\sum_{p\\in Q_t} T_{k(p)}$.\n",
    "\n",
    "We call it the <i>Packet Scheduling Problem</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2. MDP for Packet Scheduling</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.1. Mathematical Model </h3>\n",
    "\n",
    "The first step is to represent the problem as an MDP with the following components: <br>\n",
    "- Set of states: $\\mathcal S=\\{\\mathbf{s}=(n_1,n_2,\\dots,n_K)\\}_{n_i=1,\\dots,N_i, i=1,\\dots,K}$ <br>\n",
    "- Set of actions $\\mathcal A=\\{k\\}_{k=1,\\dots,K}$ <br>\n",
    "- Discount Factor: $\\gamma$\n",
    "- The MDP has an initial state $\\mathbf{s}_0=(N_1,N_2,\\dots,N_K)$ and a terminal (absorbing) state $\\mathbf{s}'=(0,0,\\dots, 0)$\n",
    "\n",
    "The action of transmitting a packet from class $k$ is represented by the following transformation: <br>\n",
    "\n",
    "$T_k: \\mathcal S \\rightarrow \\mathcal S$, such that $T_k(\\mathbf{s}) = \\mathbf{s} \\ominus \\mathbf{e}_k$,<br>\n",
    "\n",
    "where <br>\n",
    "- $\\mathbf{e}_k \\in \\{0,1\\}^{1\\times K}$ is a vector whose entry $k$ is $1$ and all other entries are $0$, and<br>\n",
    "- $\\ominus$ operator is an adapted vectorial subtraction operator, where any negative outcome is converted to zero.\n",
    "\n",
    "For example, we have $K=2$ classes and $N_1=N_2=3$ packets for each class. From the initial state $\\mathbf{s}_0=(N_1,N_2) = (3,3)$ , if we choose to transmit a packet from class $1$, we apply the transformation $T_1(\\cdot)$ as follows, <br>\n",
    "\n",
    "$T_1(\\mathbf{s}_0= (3,3)) = \\mathbf{s}_0 \\ominus \\mathbf{e}_1 = (3,3) \\ominus (1,0) = (2,3)$. <br>\n",
    "\n",
    "Therefore, if we are in state $\\mathbf{s}_0=(N_1,N_2) = (3,3)$ and transmit a packet from class $1$, then we land at state $\\mathbf{s}'=(2,3)$.<br>\n",
    "On the other hand, if we are in state $s=(3,0)$ and decide to transmit a packet from class $2$, then $T_2(\\mathbf{s}= (3,0)) = (3,0) \\ominus (0,1) = (3,0)$, i.e., no changes.\n",
    "\n",
    "\n",
    "In this activity, all transmissions (actions) lead deterministically to the corresponding immediate smaller state, we have that the transition probabilities are:<br> \n",
    "\n",
    "$\\mathcal P_{s,s'}^{a=T_k} = \\begin{cases} 1, & \\text{ if } T_k(s)=s' \\\\ 0, & \\text{otherwise}\\end{cases}$,  $\\forall s,s'\\in\\mathcal{S}, \\forall a\\in\\mathcal{A}$<br>\n",
    "\n",
    "<b><u>Exercise</u></b>: Draw the MDP for the Packet Scheduling Problem for $K=2$ classes of priority."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.2. Computational Model </h3>\n",
    "\n",
    "The code below implements an MDP Python class for $K=2$ priority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self, classes=[3, 3]):        \n",
    "        self.K = 2              # Number of classes\n",
    "        self.N = classes        # Number of packets of each class\n",
    "\n",
    "        # set of states\n",
    "        self.S = [(i,j) for i in range(self.N[0],-1,-1) for j in range(self.N[1],-1,-1)]\n",
    "        # self.S = [(i,j) for i in range(self.N[0]+1) for j in range(self.N[1]+1)]\n",
    "\n",
    "        # set of actions: Tx1 = transmits packet of class 1; Tx2 = transmits packet of class 2\n",
    "        self.A  = [\"Tx1\", \"Tx2\"]                                            \n",
    "\n",
    "        # definition of transition probabilities\n",
    "        self.P = { (s, ss, a): 1 if self.transmit(s,a) == ss else 0 for s in self.S for ss in self.S for a in self.A}  \n",
    "\n",
    "        # definition of transmission delay of each class\n",
    "        self.packet_size = 1500*8                                               # 1500 Bytes\n",
    "        self.data_rate = [100e6, 10e6]                                          # Rate 1: 100Mbps, Rate 2: 10Mbps\n",
    "        self.T = [self.packet_size / rate * 10**3 for rate in self.data_rate]   # Transmission delay [ms]\n",
    "\n",
    "        # generate queue with N1 and N2 packets randomly arranged\n",
    "        self.Q = random.sample([1] * self.N[0] + [2] * self.N[1], self.N[0] + self.N[1])\n",
    "        \n",
    "    # support function \"transmission\" that help define the transition probabilities Tp and rewards R\n",
    "    def transmit(self, s, a):\n",
    "        if s == (0,0):\n",
    "            return s\n",
    "        if a == \"Tx1\":\n",
    "            return (max(0, s[0]-1), s[1])\n",
    "        if a == \"Tx2\":\n",
    "            return (s[0], max(0, s[1]-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>Exercise</u></b>: <br>\n",
    "- The set of rewards $\\{\\mathcal{R}_{s}^a\\}_{a\\in\\mathcal{A}, s\\in\\mathcal{S}}$ is still missing from the MDP definition in Section 2.1. How would you choose the reward for this activity? Recall that there must be a reward for every action you take at every state. Explain your reasoning and add it the computational model in the code below.<br>\n",
    "- What about the discount factor $\\gamma$? Is there an appropriate value? Explain your reasoning and add it the computational model in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 1, 1, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "# Instance of MDP object\n",
    "mdp = MDP()\n",
    "print(mdp.Q)\n",
    "\n",
    "# Rewards\n",
    "# TODO: add the definition of reward\n",
    "mdp.R = {}\n",
    "\n",
    "# Discount factor, gamma\n",
    "# TODO: Exercise 3: add value for the discount factor\n",
    "mdp.g = 0.0        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3. Testing Scheduling Policies </h2>\n",
    "\n",
    "Now, we will evaluate the performance of different policies.\n",
    "\n",
    "<h3> 3.1. Policy Evaluation </h3>\n",
    "\n",
    "The method we will use is the Policy Evaluation (PE), which relies on the iterative application of Bellman Expectation Equation: <br>\n",
    "\n",
    "$v_t(s) = \\sum_{a\\in\\mathcal{A}} \\pi(a|\\,s) \\left[\\mathcal{R}_s^a + \\gamma \\sum_{s'\\in\\mathcal{S}} \\mathcal{P}_{ss'}^a\\cdot v_{t-1}(s')\\right]$\n",
    "\n",
    "<b><u>Exercise</b></u>: Use the Bellman Expectation Equation to finish the implementation of the PE function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Evaluation Algorithm\n",
    "# Input:  MDP parameters (states S, actions A, rewards R, transition probabilities Tp)\n",
    "#         policy Pi\n",
    "#         initial value v0\n",
    "#         maximum number of iterations max_iter\n",
    "# Output: list of the max_iter values for the states (last item of the list estimates v_pi)\n",
    "def policy_eval(mdp, Pi, v0, max_iter=100):\n",
    "    ## TODO: implement PE\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.2. RANDOM Policy </h3>\n",
    "\n",
    "The first policy we will try is the RANDOM policy, which selects the next action uniformly at random from the set of available actions at every state.\n",
    "\n",
    "The implementation of a RANDOM policy is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the values of the random policy\n",
      "-5.19750\t-4.53750\t-4.12500\t-3.96000\n",
      "-4.53750\t-3.63000\t-2.97000\t-2.64000\n",
      "-4.12500\t-2.97000\t-1.98000\t-1.32000\n",
      "-3.96000\t-2.64000\t-1.32000\t 0.00000\n"
     ]
    }
   ],
   "source": [
    "Pi_RANDOM = { (s, a): 1.0/len(mdp.A) for s in mdp.S for a in mdp.A }\n",
    "\n",
    "# evaluate policy Pi over max_iter iterations and store value functions of every iteration\n",
    "v_RANDOM = policy_eval(mdp, Pi_RANDOM, v0={s: 0 for s in mdp.S}, max_iter=1000)[-1]\n",
    "\n",
    "## print results\n",
    "print(\"These are the values of the random policy\")\n",
    "for s in mdp.S:\n",
    "    print(\" %.5f\" %v_RANDOM[s] if v_RANDOM[s]==0 else \"%.5f\" %v_RANDOM[s], end='\\n' if s[1] == 0 else '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.3. FIFO Policy </h3>\n",
    "\n",
    "In First-In, First-Out (FIFO) scheduling discipline, we process the packets in order they are organized in the queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pi_FIFO = {}\n",
    "for s in mdp.S:\n",
    "    if s == (0,0):\n",
    "        Pi_FIFO[(s, \"Tx1\")] = 1.0\n",
    "        Pi_FIFO[(s, \"Tx2\")] = 0.0\n",
    "    else:\n",
    "        for a in mdp.A:\n",
    "            Pi_FIFO[(s, a)] = 1.0  if \"Tx%d\" %mdp.Q[mdp.N[0] + mdp.N[1] - s[0] - s[1]] == a else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we evaluate the FIFO policy and are able to compare it with the RANDOM one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the values of the FIFO policy\n",
      "-3.96000\t-2.76000\t-1197.84000\t-1198.92000\n",
      "-120.00000\t-2.64000\t-1198.92000\t-1200.00000\n",
      "-120.00000\t-2.52000\t-1200.00000\t-1200.00000\n",
      "-120.00000\t-2.40000\t-1.20000\t 0.00000\n"
     ]
    }
   ],
   "source": [
    "# evaluate policy Pi over max_iter iterations and store value functions of every iteration\n",
    "v_FIFO = policy_eval(mdp, Pi_FIFO, v0={s: 0 for s in mdp.S}, max_iter=1000)[-1]\n",
    "\n",
    "## print results\n",
    "print(\"These are the values of the FIFO policy\")\n",
    "for s in mdp.S:\n",
    "    print(\" %.5f\" %v_FIFO[s] if v_FIFO[s]==0 else \"%.5f\" %v_FIFO[s], end='\\n' if s[1] == 0 else '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 5. Finding the Optimal Scheduling Policy </h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5.1. Value Iteration</h3>\n",
    "\n",
    "The method we will use is called the Value Iteration (VI), which relies on the iterative application of Bellman Optimality Equation: <br>\n",
    "\n",
    "$v_t(s) = \\max_{a\\in\\mathcal{A}} \\left[\\mathcal{R}_s^a + \\gamma \\sum_{s'\\in\\mathcal{S}} \\mathcal{P}_{ss'}^a\\cdot v_{t-1}(s')\\right]$\n",
    "\n",
    "<b><u>Exercise:</b></u> Use the Bellman Optimality Equation to finish the implementation of the VI function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration Algorithm\n",
    "# Input:    MDP parameters (states S, actions A, etc.)\n",
    "# Output:   (Estimate of the) Optimal values\n",
    "#           Optimal policy\n",
    "def value_iteration(mdp, max_iter=100):\n",
    "    # TODO: Implement VI\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5.2. Testing Value Iteration </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the values of the FIFO policy\n",
      "-3.96000\t-2.76000\t-1197.84000\t-1198.92000\n",
      "-120.00000\t-2.64000\t-1198.92000\t-1200.00000\n",
      "-120.00000\t-2.52000\t-1200.00000\t-1200.00000\n",
      "-120.00000\t-2.40000\t-1.20000\t 0.00000\n",
      "\n",
      "These are the values of the Greedy policy\n",
      "-3.96000\t-2.76000\t-1.56000\t-0.36000\n",
      "-3.84000\t-2.64000\t-1.44000\t-0.24000\n",
      "-3.72000\t-2.52000\t-1.32000\t-0.12000\n",
      "-3.60000\t-2.40000\t-1.20000\t 0.00000\n"
     ]
    }
   ],
   "source": [
    "# print optimal value obtained by Policy Iteration\n",
    "print(\"These are the values of the FIFO policy\")\n",
    "for s in mdp.S:\n",
    "    print(\" %.5f\" %v_FIFO[s] if v_FIFO[s]==0 else \"%.5f\" %v_FIFO[s], end='\\n' if s[1] == 0 else '\\t')\n",
    "    \n",
    "# compute optimal value for the MDP\n",
    "v, Pi_OPT = value_iteration(mdp, max_iter=1000)\n",
    "v_OPT = v[-1]\n",
    "\n",
    "# print optimal value obtained by Value Iteration\n",
    "print(\"\\nThese are the values of the Greedy policy\")\n",
    "for s in mdp.S:\n",
    "    print(\" %.5f\" %v_OPT[s] if v_OPT[s]==0 else \"%.5f\" %v_OPT[s], end='\\n' if s[1] == 0 else '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>Exercise:</b></u> Using the results obtained from the application of value iteration, answer: What is the optimal policy for the Scheduling Problem?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
