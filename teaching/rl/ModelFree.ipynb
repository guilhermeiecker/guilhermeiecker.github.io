{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Problem Definition</h1>\n",
    "\n",
    "In this exercise, we will find the optimal poliy for a GridWorld's instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> GridWorld's instance rules </h2>\n",
    "\n",
    "These exercises are based on the Gridworld game, which comprises the following rules:<br>\n",
    "- There is a squared grid with $4$ tiles in each orientation. <br>\n",
    "- The player (agent) starts in an initial tile and is free to move throughout the grid in 4 different directions: North (N), South (S), East (E), and West (W). <br>\n",
    "- At the boundary tiles, an attempt to move outside the grid results in returning to the current tile. <br>\n",
    "- A few tiles in the grid, called \"walls\", are not accessible and have the same effect as the grid's boundary. <br>\n",
    "- A few tiles in the grid, called \"crowd\", will cause severe time penalties when being traversed. <br>\n",
    "- There is exactly one terminal tile. <br>\n",
    "- Our goal is to reach a final tile as quick as possible.\n",
    "\n",
    "We will use the following grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "|\u001b[96m(1,1)\t\u001b[0m|(1,2)\t|(1,3)\t|(1,4)\t|\n",
      "|\u001b[96mstart\t\u001b[0m|     \t|     \t|     \t|\n",
      "---------------------------------\n",
      "|####\t|\u001b[91m(2,2)\t\u001b[0m|####\t|(2,4)\t|\n",
      "|####\t|\u001b[91mcrowd\t\u001b[0m|####\t|     \t|\n",
      "---------------------------------\n",
      "|\u001b[92m(3,1)\t\u001b[0m|(3,2)\t|####\t|(3,4)\t|\n",
      "|\u001b[92mend\t\u001b[0m|     \t|####\t|     \t|\n",
      "---------------------------------\n",
      "|\u001b[91m(4,1)\t\u001b[0m|(4,2)\t|(4,3)\t|(4,4)\t|\n",
      "|\u001b[91mcrowd\t\u001b[0m|     \t|     \t|     \t|\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# print the grid with the optimal values and policies obtained by Value Iteration\n",
    "print(\"---------------------------------\")\n",
    "print(\"|\\033[96m%s\\t\\033[0m|%s\\t|%s\\t|%s\\t|\"      %(\"(1,1)\", \"(1,2)\", \"(1,3)\", \"(1,4)\"))\n",
    "print(\"|\\033[96m%s\\t\\033[0m|%s\\t|%s\\t|%s\\t|\"      %(\"start\", \"     \", \"     \", \"     \"))\n",
    "print(\"---------------------------------\")\n",
    "print(\"|####\\t|\\033[91m%s\\t\\033[0m|####\\t|%s\\t|\"  %(\"(2,2)\", \"(2,4)\"))\n",
    "print(\"|####\\t|\\033[91m%s\\t\\033[0m|####\\t|%s\\t|\"  %(\"crowd\", \"     \"))\n",
    "print(\"---------------------------------\")\n",
    "print(\"|\\033[92m%s\\t\\033[0m|%s\\t|####\\t|%s\\t|\"    %(\"(3,1)\", \"(3,2)\", \"(3,4)\"))\n",
    "print(\"|\\033[92m%s\\t\\033[0m|%s\\t|####\\t|%s\\t|\"    %(\"end\",   \"     \", \"     \"))\n",
    "print(\"---------------------------------\")\n",
    "print(\"|\\033[91m%s\\t\\033[0m|%s\\t|%s\\t|%s\\t|\"      %(\"(4,1)\", \"(4,2)\", \"(4,3)\", \"(4,4)\"))\n",
    "print(\"|\\033[91m%s\\t\\033[0m|%s\\t|%s\\t|%s\\t|\"      %(\"crowd\", \"     \", \"     \", \"     \"))\n",
    "print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> MDP model </h2>\n",
    "\n",
    "The first step is to represent the problem as an MDP with the following components: <br>\n",
    "- Set of states: $\\mathcal S=\\{(x,y)\\}_{x=1,\\dots,K, y=1,\\dots,K}$ (each tile is a state) <br>\n",
    "- According to the provided grid, the states are:\n",
    "  - Initial state: $(1,1)$ \n",
    "  - Terminal state: $(3,1)$\n",
    "  - \"Wall\" states: $(2,1)$, $(2,3)$, $(3,3)$\n",
    "  - \"Crowd\" states: $(4,1)$, $(2,2)$\n",
    "  - All other states are \"Simple\"\n",
    "- Set of actions $\\mathcal A=\\{\\uparrow, \\downarrow, \\rightarrow, \\leftarrow\\}$ (the same for each state) <br>\n",
    "- Each action $a\\in\\mathcal A$ is a function mapping one state to another, i.e., $a: \\mathcal S \\rightarrow \\mathcal S$, and can be defined as follows:\n",
    "  - $\\uparrow(x,y) = \\begin{cases} (x,y), & \\text{if (1) state } (x,y) \\text{ is terminal or (2) state } (x,y-1) \\text{ is outside the grid or a ``wall'' tile} \\\\ (x,y-1), & \\text{otherwise} \\end{cases}$\n",
    "  - $\\downarrow(x,y) = \\begin{cases} (x,y), & \\text{if (1) state } (x,y) \\text{ is terminal or (2) state } (x,y+1) \\text{ is outside the grid or a ``wall'' tile} \\\\ (x,y+1), & \\text{otherwise} \\end{cases}$\n",
    "  - $\\rightarrow(x,y) = \\begin{cases} (x,y), & \\text{if (1) state } (x,y) \\text{ is terminal or (2) state } (x+1,y) \\text{ is outside the grid or a ``wall'' tile} \\\\ (x+1,y), & \\text{otherwise} \\end{cases}$\n",
    "  - $\\leftarrow(x,y) = \\begin{cases} (x,y), & \\text{if (1) state } (x,y) \\text{ is terminal or (2) state } (x-1,y) \\text{ is outside the grid or a ``wall'' tile} \\\\ (x-1,y), & \\text{otherwise} \\end{cases}$\n",
    "- Discount Factor: $\\gamma$\n",
    "- Because, in this exercise, all actions lead deterministically to a single state, we consider: <br>\n",
    "  - Transition probabilities are $1$ if the next state is a result of the action and $0$ otherwise, i.e., $\\mathcal P_{ss'}^{a} = \\begin{cases} 1, & \\text{ if } a(s)=s' \\\\ 0, & \\text{otherwise}\\end{cases}$<br>\n",
    "  - There are not random factors affecting the rewards so they have constant values, i.e., $\\mathcal R_s^a = \\mathbb{E} [ R_{t+1} |\\, S_t=s, A_t=a] = R_s^a$, where $R_s^a=\\begin{cases} -1 & \\text{if } a(s) \\text{ is a \"Simple\" tile} \\\\-5 & \\text{if } a(s) \\text{ is a \"Crowd\" tile} \\\\ 0 & \\text{if } s \\text{ is a \"Terminal\" tile (regardless of the action } a) \\end{cases}$\n",
    "\n",
    "The MDP can be implemented as a Python class as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self, grid_size=4, gamma=1.0):\n",
    "        self.k = grid_size  # grid side\n",
    "        self.g = gamma      # discount factor\n",
    "\n",
    "        # set of states\n",
    "        self.S = [(i+1,j+1) for i in range(self.k) for j in range(self.k)]\n",
    "        self.walls = [(2,1), (2,3), (3,3)]\n",
    "        for wall in self.walls:\n",
    "            self.S.remove(wall)\n",
    "\n",
    "        self.terminal = [(3,1)]\n",
    "        self.crowds = [(4,1), (2,2)]\n",
    "        self.simple = [s for s in self.S]\n",
    "        for pair in self.terminal + self.crowds:\n",
    "            self.simple.remove(pair)\n",
    "\n",
    "        # set of actions\n",
    "        self.A  = [\"up\", \"down\", \"right\", \"left\"]                                            \n",
    "\n",
    "        # definition of transition probabilities\n",
    "        self.Tp = {}\n",
    "        for s in self.S:\n",
    "            for ss in self.S:\n",
    "                for a in self.A:\n",
    "                    if self.move(s,a) == ss:\n",
    "                        self.Tp[(s,ss,a)] = 1 \n",
    "                    else:\n",
    "                        self.Tp[(s,ss,a)] = 0 \n",
    "\n",
    "        # definition of expected reward R\n",
    "        self.R = {}\n",
    "        for s in self.S:\n",
    "            for a in self.A:\n",
    "                if s in self.terminal:\n",
    "                    self.R[(s,a)] = 0\n",
    "                    continue\n",
    "                if self.move(s, a) in self.simple + self.terminal:\n",
    "                    self.R[(s,a)] = -1\n",
    "                if self.move(s, a) in self.crowds:\n",
    "                    self.R[(s,a)] = -5\n",
    "\n",
    "   \n",
    "    # support function \"action\" that help define the transition probabilities Tp\n",
    "    def move(self, s, a):   \n",
    "        if s in self.terminal:\n",
    "            return s\n",
    "        if a == \"up\":\n",
    "            return s if (s[0]-1,s[1]) in self.walls or s[0]-1 == 0 else (s[0]-1,s[1])\n",
    "        if a == \"down\":\n",
    "            return s if (s[0]+1,s[1]) in self.walls or s[0]+1 == self.k + 1 else (s[0]+1,s[1])\n",
    "        if a == \"right\":\n",
    "            return s if (s[0],s[1]+1) in self.walls or s[1]+1 == self.k + 1 else (s[0],s[1]+1)\n",
    "        if a == \"left\":\n",
    "            return s if (s[0],s[1]-1) in self.walls or s[1]-1 == 0 else (s[0],s[1]-1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Solution Methods</h1>\n",
    "\n",
    "Now, we are going to use different RL methods to solve the problem.\n",
    "As a reference, we will start with a model-based technique called Value Iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Model-Based Solution: Value Iteration </h2>\n",
    "\n",
    "Here is the code for the Value Iteration algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration Algorithm\n",
    "# Based on Value Iteration Algorithm (Sutton and Barto - Section 4.4)\n",
    "# New feature:\n",
    "#    - additional stop criterion based on maximum number of iteration\n",
    "# Input:  MDP parameters (states S, actions A)\n",
    "# Output: (Estimate of the) Optimal values\n",
    "def value_iteration(mdp, theta=1.0e-6, max_iter=10):\n",
    "    # initialization\n",
    "    v = {s: 0.0 if s in mdp.terminal else -100 for s in mdp.S}\n",
    "\n",
    "    t = 0\n",
    "    Delta = 0\n",
    "    while(True):\n",
    "        t += 1\n",
    "        for s in mdp.S:\n",
    "            v_temp = v[s]\n",
    "            v[s] = max(sum(mdp.Tp[(s,ss,a)]*(mdp.R[(s,a)] + mdp.g*v[ss]) for ss in mdp.S) for a in mdp.A)\n",
    "            Delta = max(Delta, abs(v_temp - v[s]))\n",
    "        if t>max_iter or Delta<theta:\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    Pi = {}\n",
    "    for s in mdp.S:\n",
    "        max_value = -100000.0\n",
    "        max_action = \"\"\n",
    "        for a in mdp.A:\n",
    "            value = 0.0\n",
    "            for ss in mdp.S:\n",
    "                value += mdp.Tp[(s, ss, a)] * (mdp.R[(s, a)] + mdp.g*v[ss])\n",
    "            if value >= max_value:\n",
    "                max_value = value\n",
    "                max_action = a\n",
    "        for a in mdp.A:\n",
    "            Pi[(s,a)] = 1.0 if a == max_action else 0\n",
    "\n",
    "    return v, Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "|\u001b[96m-8.0\t\u001b[0m|-7.0\t|-8.0\t|-7.0\t|\n",
      "|\u001b[96mright\t\u001b[0m|down\t|left\t|down\t|\n",
      "---------------------------------\n",
      "|####\t|\u001b[91m-2.0\t\u001b[0m|####\t|-6.0\t|\n",
      "|####\t|\u001b[91mdown\t\u001b[0m|####\t|down\t|\n",
      "---------------------------------\n",
      "|\u001b[92m0.0\t\u001b[0m|-1.0\t|####\t|-5.0\t|\n",
      "|\u001b[92mleft\t\u001b[0m|left\t|####\t|down\t|\n",
      "---------------------------------\n",
      "|\u001b[91m-1.0\t\u001b[0m|-2.0\t|-3.0\t|-4.0\t|\n",
      "|\u001b[91mup\t\u001b[0m|up\t|left\t|left\t|\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "mdp = MDP(gamma=1.0)\n",
    "theta = 1.0e-6\n",
    "max_iteration = 1000\n",
    "\n",
    "# compute optimal value v(s) and optimal policy Pi(s) for each state s of the MDP\n",
    "v, Pi_VI = value_iteration(mdp, theta, max_iteration)\n",
    "\n",
    "# print the grid with the optimal values and policies obtained by Value Iteration\n",
    "Pi = {}\n",
    "for s in mdp.S:\n",
    "    for a in mdp.A:\n",
    "        if Pi_VI[(s,a)] == 1:\n",
    "            Pi[s] = a\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "print(\"|\\033[96m%s\\t\\033[0m|%s\\t|%s\\t|%s\\t|\"      %(v[(1,1)], v[(1,2)], v[(1,3)], v[(1,4)]))\n",
    "print(\"|\\033[96m%s\\t\\033[0m|%s\\t|%s\\t|%s\\t|\"      %(Pi[(1,1)], Pi[(1,2)], Pi[(1,3)], Pi[(1,4)]))\n",
    "print(\"---------------------------------\")\n",
    "print(\"|####\\t|\\033[91m%s\\t\\033[0m|####\\t|%s\\t|\"  %(v[(2,2)], v[(2,4)]))\n",
    "print(\"|####\\t|\\033[91m%s\\t\\033[0m|####\\t|%s\\t|\"  %(Pi[(2,2)], Pi[(2,4)]))\n",
    "print(\"---------------------------------\")\n",
    "print(\"|\\033[92m%s\\t\\033[0m|%s\\t|####\\t|%s\\t|\"    %(v[(3,1)], v[(3,2)], v[(3,4)]))\n",
    "print(\"|\\033[92m%s\\t\\033[0m|%s\\t|####\\t|%s\\t|\"    %(Pi[(3,1)], Pi[(3,2)], Pi[(3,4)]))\n",
    "print(\"---------------------------------\")\n",
    "print(\"|\\033[91m%s\\t\\033[0m|%s\\t|%s\\t|%s\\t|\"      %(v[(4,1)], v[(4,2)], v[(4,3)], v[(4,4)]))\n",
    "print(\"|\\033[91m%s\\t\\033[0m|%s\\t|%s\\t|%s\\t|\"      %(Pi[(4,1)], Pi[(4,2)], Pi[(4,3)], Pi[(4,4)]))\n",
    "print(\"---------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Model-free Solutions</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to explore a few approaches to solve the problem without the MDP.\n",
    "We start by introducing the GridWorld simulator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> GridWorld Simulator </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, init_loc=(1,1)):\n",
    "        self.env = env\n",
    "        self.loc = init_loc     # agent's current location. Initial location set to (1,1)\n",
    "\n",
    "        # set of states\n",
    "        self.S = [(i+1,j+1) for i in range(self.env.k) for j in range(self.env.k)]\n",
    "\n",
    "        # set of actions\n",
    "        self.A = [\"up\", \"down\", \"right\", \"left\"] \n",
    "        self.m = len(self.A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, grid_size=4):\n",
    "        self.k = grid_size  # grid side\n",
    "\n",
    "        # set of states\n",
    "        self.tiles = [(i+1,j+1) for i in range(self.k) for j in range(self.k)]\n",
    "        self.walls = [(2,1), (2,3), (3,3)]\n",
    "        for wall in self.walls:\n",
    "            self.tiles.remove(wall)\n",
    "\n",
    "        self.terminal = [(3,1)]\n",
    "        self.crowds = [(4,1), (2,2)]\n",
    "        self.simple = [s for s in self.tiles]\n",
    "        for pair in self.terminal + self.crowds:\n",
    "            self.simple.remove(pair)\n",
    "\n",
    "        # set of actions\n",
    "        self.A  = [\"up\", \"down\", \"right\", \"left\"]                                            \n",
    "\n",
    "        # definition of observed reward\n",
    "        self.R = {}\n",
    "        for s in self.tiles:\n",
    "            for a in self.A:\n",
    "                if s in self.terminal:\n",
    "                    self.R[(s,a)] = 0\n",
    "                    continue\n",
    "                if self.move(s, a) in self.simple + self.terminal:\n",
    "                    self.R[(s,a)] = -1\n",
    "                if self.move(s, a) in self.crowds:\n",
    "                    self.R[(s,a)] = -5\n",
    "\n",
    "    # support function \"action\" that help define the observed reward\n",
    "    def move(self, s, a):   \n",
    "        if s in self.terminal:\n",
    "            return s\n",
    "        if a == \"up\":\n",
    "            return s if (s[0]-1,s[1]) in self.walls or s[0]-1 == 0 else (s[0]-1,s[1])\n",
    "        if a == \"down\":\n",
    "            return s if (s[0]+1,s[1]) in self.walls or s[0]+1 == self.k + 1 else (s[0]+1,s[1])\n",
    "        if a == \"right\":\n",
    "            return s if (s[0],s[1]+1) in self.walls or s[1]+1 == self.k + 1 else (s[0],s[1]+1)\n",
    "        if a == \"left\":\n",
    "            return s if (s[0],s[1]-1) in self.walls or s[1]-1 == 0 else (s[0],s[1]-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Monte-Carlo Learning </h3>\n",
    "\n",
    "Here is an implementation of the Monte-Carlo methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC:\n",
    "    def __init__(self, _gamma, _agent, _environment):\n",
    "        self.gamma = _gamma\n",
    "        self.agent = _agent\n",
    "        self.env = _environment\n",
    "\n",
    "    # This function simulates episodes where the agent explores the tiles of the GridWorld's instance\n",
    "    # At every time step t, we compute (s_t, a_t, r_t+1, s_t+1)\n",
    "    def generate_episode(self, policy):\n",
    "        episode = []\n",
    "        self.agent.loc = (1,1)\n",
    "        while not self.agent.loc in self.env.terminal:\n",
    "            # decide the next action based on the provided policy\n",
    "            next_action = np.random.choice(self.agent.A, p=[policy[(self.agent.loc, a)] for a in self.agent.A])\n",
    "            # collect the reward related to the performed action and add tuple (s,a,r,s') \n",
    "            episode.append((self.agent.loc, next_action, self.env.R[(self.agent.loc, next_action)], self.env.move(self.agent.loc, next_action)))\n",
    "            # move the agent to the new location based on the performed action\n",
    "            self.agent.loc = self.env.move(self.agent.loc, next_action)\n",
    "            \n",
    "        return episode\n",
    "    \n",
    "    # Monte-Carlo Prediction Algorithm\n",
    "    # Based on Every-Visit MC Prediction Algorithm (Sutton and Barto - Section 5.1) and adapted with David Silver's slides ideas\n",
    "    # Input:    - A policy to be evaluated\n",
    "    #           - Total number of episodes\n",
    "    # Output:   - estimate of the state-value functions V\n",
    "    #           - estimate of the action-value functions Q\n",
    "    def prediction(self, policy, num_episodes):\n",
    "        N_state = {s: 0 for s in self.agent.S}\n",
    "        N_action = {(s,a): 0 for s in self.agent.S for a in self.agent.A}\n",
    "        S_state = {s: 0.0 for s in self.agent.S}\n",
    "        S_action = {(s,a): 0.0 for s in self.agent.S for a in self.agent.A}\n",
    "\n",
    "        for k in range(num_episodes):\n",
    "            episode = self.generate_episode(policy)\n",
    "            G = 0.0\n",
    "            for (s,a,r,ss) in reversed(episode):\n",
    "                G = self.gamma*G + r\n",
    "                N_state[s] += 1\n",
    "                S_state[s] += G\n",
    "                N_action[(s,a)] += 1\n",
    "                S_action[(s,a)] += G\n",
    "        \n",
    "        V = {}\n",
    "        for s in self.agent.S:\n",
    "            if s in self.env.terminal:\n",
    "                V[s] = 0.0\n",
    "            else:\n",
    "                if N_state[s] > 0:\n",
    "                    V[s] = S_state[s]/N_state[s]\n",
    "                else:\n",
    "                    V[s] = -num_episodes\n",
    "\n",
    "        Q = {}\n",
    "        for s in self.agent.S:\n",
    "            for a in self.agent.A:\n",
    "                if s in self.env.terminal:\n",
    "                    Q[(s,a)] = 0.0\n",
    "                else:\n",
    "                    if N_action[(s,a)] > 0:\n",
    "                        Q[(s,a)] =  S_action[(s,a)]/N_action[(s,a)]\n",
    "                    else:\n",
    "                        Q[(s,a)] = -num_episodes\n",
    "\n",
    "        return V, Q\n",
    "    \n",
    "    # Monte-Carlo Q-Value Iteration Algorithm\n",
    "    # Based on One-Visit MC Control Algorithm (Sutton and Barto - Section 5.4)\n",
    "    # Input:    - An initial policy\n",
    "    #           - Total number of iterations for each prediction step\n",
    "    #           - Total number of episodes\n",
    "    #           - epsilon, to define the epsilon-greedy probability\n",
    "    # Output:   - estimate of the optimal policy\n",
    "    def control(self, Pi_init, num_iterations, num_episodes, epsilon):\n",
    "        Pi_opt = Pi_init\n",
    "        for i in range(num_iterations):\n",
    "            v,q = self.prediction(Pi_opt, num_episodes)\n",
    "            Pi_opt = self.epsilon_greedy(epsilon, q)\n",
    "        return Pi_opt\n",
    "    \n",
    "    def epsilon_greedy(self, epsilon, q):\n",
    "        Pi = {(s,a): 0.0 for s in self.agent.S for a in self.agent.A}\n",
    "        for s in self.agent.S:\n",
    "            a_max = \"\"\n",
    "            q_max = -10000.0\n",
    "            for a in self.agent.A:\n",
    "                if q[(s,a)] > q_max:\n",
    "                    q_max = q[(s,a)]\n",
    "                    a_max = a\n",
    "            \n",
    "            for a in self.agent.A:\n",
    "                if a == a_max:\n",
    "                    Pi[(s,a)] = epsilon/self.agent.m + 1 - epsilon\n",
    "                else:\n",
    "                    Pi[(s,a)] = epsilon/self.agent.m\n",
    "\n",
    "        return Pi\n",
    "\n",
    "gamma = 1.0\n",
    "env = Environment(grid_size=4)\n",
    "agent = Agent(env=env,  init_loc=(1,1))\n",
    "\n",
    "mc = MC(gamma, agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing MC prediction for RANDOM policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "|\u001b[96m-95.2\t\u001b[0m|-91.2\t|-97.7\t|-99.6\t|\n",
      "---------------------------------\n",
      "|####\t|\u001b[91m-72.4\t\u001b[0m|####\t|-98.2\t|\n",
      "---------------------------------\n",
      "|\u001b[92m0.0\t\u001b[0m|-41.9\t|####\t|-92.0\t|\n",
      "---------------------------------\n",
      "|\u001b[91m-29.3\t\u001b[0m|-48.6\t|-67.7\t|-81.3\t|\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "Pi_RANDOM = { (s, a): 1.0/agent.m for s in agent.S for a in agent.A } # random policy\n",
    "num_episodes = 10000                                                 # number of episodes \n",
    "v_MC, Q_MC = mc.prediction(Pi_RANDOM, num_episodes)\n",
    "\n",
    "# print the grid with the optimal values and policies obtained by Value Iteration\n",
    "print(\"---------------------------------\")\n",
    "print(\"|\\033[96m%.1f\\t\\033[0m|%.1f\\t|%.1f\\t|%.1f\\t|\"      %(v_MC[(1,1)], v_MC[(1,2)], v_MC[(1,3)], v_MC[(1,4)]))\n",
    "print(\"---------------------------------\")\n",
    "print(\"|####\\t|\\033[91m%.1f\\t\\033[0m|####\\t|%.1f\\t|\"  %(v_MC[(2,2)], v_MC[(2,4)]))\n",
    "print(\"---------------------------------\")\n",
    "print(\"|\\033[92m%.1f\\t\\033[0m|%.1f\\t|####\\t|%.1f\\t|\"    %(v_MC[(3,1)], v_MC[(3,2)], v_MC[(3,4)]))\n",
    "print(\"---------------------------------\")\n",
    "print(\"|\\033[91m%.1f\\t\\033[0m|%.1f\\t|%.1f\\t|%.1f\\t|\"      %(v_MC[(4,1)], v_MC[(4,2)], v_MC[(4,3)], v_MC[(4,4)]))\n",
    "print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing MC prediction for the Value Iteration's policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "|\u001b[96m-8.0\t\u001b[0m|-7.0\t|-10000.0\t|-10000.0\t|\n",
      "---------------------------------\n",
      "|####\t|\u001b[91m-2.0\t\u001b[0m|####\t|-10000.0\t|\n",
      "---------------------------------\n",
      "|\u001b[92m0.0\t\u001b[0m|-1.0\t|####\t|-10000.0\t|\n",
      "---------------------------------\n",
      "|\u001b[91m-10000.0\t\u001b[0m|-10000.0\t|-10000.0\t|-10000.0\t|\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10000                                                  # number of episodes \n",
    "v_MC, Q_MC = mc.prediction(Pi_VI, num_episodes)\n",
    "\n",
    "# print the grid with the optimal values and policies obtained by Value Iteration\n",
    "print(\"---------------------------------\")\n",
    "print(\"|\\033[96m%.1f\\t\\033[0m|%.1f\\t|%.1f\\t|%.1f\\t|\"      %(v_MC[(1,1)], v_MC[(1,2)], v_MC[(1,3)], v_MC[(1,4)]))\n",
    "print(\"---------------------------------\")\n",
    "print(\"|####\\t|\\033[91m%.1f\\t\\033[0m|####\\t|%.1f\\t|\"  %(v_MC[(2,2)], v_MC[(2,4)]))\n",
    "print(\"---------------------------------\")\n",
    "print(\"|\\033[92m%.1f\\t\\033[0m|%.1f\\t|####\\t|%.1f\\t|\"    %(v_MC[(3,1)], v_MC[(3,2)], v_MC[(3,4)]))\n",
    "print(\"---------------------------------\")\n",
    "print(\"|\\033[91m%.1f\\t\\033[0m|%.1f\\t|%.1f\\t|%.1f\\t|\"      %(v_MC[(4,1)], v_MC[(4,2)], v_MC[(4,3)], v_MC[(4,4)]))\n",
    "print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing MC learning for policy optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1) right -1 (1, 2)\n",
      "(1, 2) down -5 (2, 2)\n",
      "(2, 2) down -1 (3, 2)\n",
      "(3, 2) left -1 (3, 1)\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 10\n",
    "num_episodes = 1000                                                  # number of episodes\n",
    "epsilon = 0.04\n",
    "\n",
    "Pi_opt = mc.control(Pi_RANDOM, num_iterations, num_episodes, epsilon)\n",
    "v_opt, Q_opt = mc.prediction(Pi_opt, num_episodes)\n",
    "\n",
    "for (s,a,r,ss) in mc.generate_episode(Pi_opt):\n",
    "    print(s, a, r, ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Temporal-Difference Learning</h3>\n",
    "\n",
    "Here is the class implementing the TD-based methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD:\n",
    "    def __init__(self, _gamma, _agent, _environment):\n",
    "        self.gamma = _gamma\n",
    "        self.agent = _agent\n",
    "        self.env = _environment\n",
    "\n",
    "    # This function simulates episodes where the agent explores the tiles of the GridWorld's instance\n",
    "    # At every time step t, we compute (s_t, a_t, r_t+1, s_t+1)\n",
    "    def generate_episode(self, policy):\n",
    "        episode = []\n",
    "        self.agent.loc = (1,1)\n",
    "        while not self.agent.loc in self.env.terminal:\n",
    "            # decide the next action based on the provided policy\n",
    "            next_action = np.random.choice(self.agent.A, p=[policy[(self.agent.loc, a)] for a in self.agent.A])\n",
    "            # collect the reward related to the performed action and add tuple (s,a,r,s') \n",
    "            episode.append((self.agent.loc, next_action, self.env.R[(self.agent.loc, next_action)], self.env.move(self.agent.loc, next_action)))\n",
    "            # move the agent to the new location based on the performed action\n",
    "            self.agent.loc = self.env.move(self.agent.loc, next_action)\n",
    "            \n",
    "        return episode\n",
    "        \n",
    "    # TD(0) Prediction  Algorithm\n",
    "    # Based on Every-Visit MC Prediction Algorithm (Sutton and Barto - Section 6.1)\n",
    "    # Input:    - A policy to be evaluated, pi\n",
    "    #           - Learning rate, alpha\n",
    "    #           - Total number of episodes, num_episodes\n",
    "    # Output:   - estimate of the state-value functions V\n",
    "    def prediction(self, pi, alpha, num_episodes):\n",
    "        V = {s: 0 if s in self.env.terminal else -100 for s in self.agent.S}\n",
    "\n",
    "        for k in range(num_episodes):\n",
    "            self.agent.loc = (1,1)\n",
    "            while not self.agent.loc in self.env.terminal: \n",
    "                S = self.agent.loc\n",
    "                A = np.random.choice(self.agent.A, p=[pi[(S, a)] for a in self.agent.A])\n",
    "                R = self.env.R[(S, A)]\n",
    "                Sprime = self.env.move(S, A)\n",
    "\n",
    "                V[S] = V[S] + alpha*(R + self.gamma*V[Sprime] - V[S])\n",
    "\n",
    "                self.agent.loc = Sprime\n",
    "        \n",
    "        return V\n",
    "    \n",
    "    def sarsa(self, alpha, epsilon, num_episodes):\n",
    "        Q = {(s,a): 0.0 if s in self.env.terminal else -100.0 for s in self.agent.S for a in self.agent.A}\n",
    "\n",
    "        for k in range(num_episodes):\n",
    "            S = (1,1)\n",
    "            A = np.random.choice(self.agent.A, p=self.epsilon_greedy(S, Q, epsilon))\n",
    "\n",
    "            while not S in self.env.terminal:               \n",
    "                R = self.env.R[(S, A)]\n",
    "                Sprime = self.env.move(S, A)\n",
    "                Aprime = np.random.choice(self.agent.A, p=self.epsilon_greedy(Sprime, Q, epsilon))\n",
    "\n",
    "                Q[(S,A)] += alpha*(R + self.gamma*Q[(Sprime,Aprime)] - Q[(S,A)])\n",
    "                \n",
    "                S = Sprime   \n",
    "                A = Aprime\n",
    "        \n",
    "        return Q\n",
    "\n",
    "    def q_learning(self, alpha, epsilon, num_episodes):\n",
    "        Q = {(s,a): 0.0 if s in self.env.terminal else -100.0 for s in self.agent.S for a in self.agent.A}\n",
    "\n",
    "        for k in range(num_episodes):\n",
    "            S = (1,1)\n",
    "            while not S in self.env.terminal:      \n",
    "                A = np.random.choice(self.agent.A, p=self.epsilon_greedy(S, Q, epsilon))\n",
    "                R = self.env.R[(S, A)]\n",
    "                Sprime = self.env.move(S, A)\n",
    "\n",
    "                Q[(S,A)] += alpha*(R + self.gamma*max( Q[(Sprime,a)] - Q[(S,A)] for a in self.agent.A))\n",
    "\n",
    "                S = Sprime\n",
    "        \n",
    "        return Q        \n",
    "\n",
    "\n",
    "    def epsilon_greedy(self, S, Q, epsilon):\n",
    "        a_max = \"\"\n",
    "        q_max = -10000.0\n",
    "        for a in self.agent.A:\n",
    "            if Q[(S,a)] > q_max:\n",
    "                q_max = Q[(S,a)]\n",
    "                a_max = a\n",
    "        pi = {a: epsilon/self.agent.m + 1 - epsilon if a == a_max else  epsilon/self.agent.m for a in self.agent.A}\n",
    "        \n",
    "        return [pi[a] for a in self.agent.A]\n",
    "    \n",
    "\n",
    "gamma = 1.0\n",
    "env = Environment(grid_size=4)\n",
    "agent = Agent(env=env,  init_loc=(1,1))\n",
    "\n",
    "td = TD(gamma, agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing TD Prediction with RANDOM policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "|\u001b[96m-28.0\t\u001b[0m|-117.0\t|-78.0\t|-47.0\t|\n",
      "---------------------------------\n",
      "|####\t|\u001b[91m-20.0\t\u001b[0m|####\t|-28.0\t|\n",
      "---------------------------------\n",
      "|\u001b[92m0.0\t\u001b[0m|-1.0\t|####\t|-39.0\t|\n",
      "---------------------------------\n",
      "|\u001b[91m-1.0\t\u001b[0m|-20.0\t|-9.0\t|-24.0\t|\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "alpha = 1.0\n",
    "num_episodes = 10000\n",
    "v_TD_RANDOM = td.prediction(Pi_RANDOM, alpha, num_episodes)\n",
    "\n",
    "# print the grid with the optimal values and policies obtained by Value Iteration\n",
    "print(\"---------------------------------\")\n",
    "print(\"|\\033[96m%.1f\\t\\033[0m|%.1f\\t|%.1f\\t|%.1f\\t|\"      %(v_TD_RANDOM[(1,1)], v_TD_RANDOM[(1,2)], v_TD_RANDOM[(1,3)], v_TD_RANDOM[(1,4)]))\n",
    "print(\"---------------------------------\")\n",
    "print(\"|####\\t|\\033[91m%.1f\\t\\033[0m|####\\t|%.1f\\t|\"  %(v_TD_RANDOM[(2,2)], v_TD_RANDOM[(2,4)]))\n",
    "print(\"---------------------------------\")\n",
    "print(\"|\\033[92m%.1f\\t\\033[0m|%.1f\\t|####\\t|%.1f\\t|\"    %(v_TD_RANDOM[(3,1)], v_TD_RANDOM[(3,2)], v_TD_RANDOM[(3,4)]))\n",
    "print(\"---------------------------------\")\n",
    "print(\"|\\033[91m%.1f\\t\\033[0m|%.1f\\t|%.1f\\t|%.1f\\t|\"      %(v_TD_RANDOM[(4,1)], v_TD_RANDOM[(4,2)], v_TD_RANDOM[(4,3)], v_TD_RANDOM[(4,4)]))\n",
    "print(\"---------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing TD Prediction with Value Iteration Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "|\u001b[96m-8.0\t\u001b[0m|-7.0\t|-100.0\t|-100.0\t|\n",
      "---------------------------------\n",
      "|####\t|\u001b[91m-2.0\t\u001b[0m|####\t|-100.0\t|\n",
      "---------------------------------\n",
      "|\u001b[92m0.0\t\u001b[0m|-1.0\t|####\t|-100.0\t|\n",
      "---------------------------------\n",
      "|\u001b[91m-100.0\t\u001b[0m|-100.0\t|-100.0\t|-100.0\t|\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "alpha = 1.0\n",
    "num_episodes = 10000\n",
    "v_TD_VI = td.prediction(Pi_VI, alpha, num_episodes)\n",
    "\n",
    "# print the grid with the optimal values and policies obtained by Value Iteration\n",
    "print(\"---------------------------------\")\n",
    "print(\"|\\033[96m%.1f\\t\\033[0m|%.1f\\t|%.1f\\t|%.1f\\t|\"      %(v_TD_VI[(1,1)], v_TD_VI[(1,2)], v_TD_VI[(1,3)], v_TD_VI[(1,4)]))\n",
    "print(\"---------------------------------\")\n",
    "print(\"|####\\t|\\033[91m%.1f\\t\\033[0m|####\\t|%.1f\\t|\"  %(v_TD_VI[(2,2)], v_TD_VI[(2,4)]))\n",
    "print(\"---------------------------------\")\n",
    "print(\"|\\033[92m%.1f\\t\\033[0m|%.1f\\t|####\\t|%.1f\\t|\"    %(v_TD_VI[(3,1)], v_TD_VI[(3,2)], v_TD_VI[(3,4)]))\n",
    "print(\"---------------------------------\")\n",
    "print(\"|\\033[91m%.1f\\t\\033[0m|%.1f\\t|%.1f\\t|%.1f\\t|\"      %(v_TD_VI[(4,1)], v_TD_VI[(4,2)], v_TD_VI[(4,3)], v_TD_VI[(4,4)]))\n",
    "print(\"---------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing (On-Policy) TD Control with Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1) right -1 (1, 2)\n",
      "(1, 2) down -5 (2, 2)\n",
      "(2, 2) down -1 (3, 2)\n",
      "(3, 2) left -1 (3, 1)\n"
     ]
    }
   ],
   "source": [
    "alpha = 1.0\n",
    "epsilon = 0.004\n",
    "num_episodes = 1000\n",
    "\n",
    "Q = td.sarsa(alpha, epsilon, num_episodes)\n",
    "\n",
    "Pi_Sarsa = {}\n",
    "for s in td.agent.S:\n",
    "    a_max = \"\"\n",
    "    q_max = -10000.0\n",
    "    for a in td.agent.A:\n",
    "        if Q[(s,a)] > q_max:\n",
    "            q_max = Q[(s,a)]\n",
    "            a_max = a\n",
    "    for a in td.agent.A:\n",
    "        Pi_Sarsa[(s,a)] = 1.0 if a == a_max else 0.0\n",
    "\n",
    "for (s,a,r,ss) in td.generate_episode(Pi_Sarsa):\n",
    "    print(s, a, r, ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing (Off-Policy) TD Control with Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1) right -1 (1, 2)\n",
      "(1, 2) down -5 (2, 2)\n",
      "(2, 2) down -1 (3, 2)\n",
      "(3, 2) left -1 (3, 1)\n"
     ]
    }
   ],
   "source": [
    "alpha = 1.0\n",
    "epsilon = 0.004\n",
    "num_episodes = 1000\n",
    "\n",
    "Q = td.q_learning(alpha, epsilon, num_episodes)\n",
    "\n",
    "Pi_QLearning = {}\n",
    "for s in td.agent.S:\n",
    "    a_max = \"\"\n",
    "    q_max = -10000.0\n",
    "    for a in td.agent.A:\n",
    "        if Q[(s,a)] > q_max:\n",
    "            q_max = Q[(s,a)]\n",
    "            a_max = a\n",
    "    for a in td.agent.A:\n",
    "        Pi_QLearning[(s,a)] = 1.0 if a == a_max else 0.0\n",
    "\n",
    "for (s,a,r,ss) in mc.generate_episode(Pi_QLearning):\n",
    "    print(s, a, r, ss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
